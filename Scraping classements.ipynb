{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On télecharge les packages dont on va avoir besoin\n",
    "import selenium #pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By  \n",
    "import time\n",
    "#BeautifulSoup , Request et Pandas\n",
    "import urllib\n",
    "import bs4\n",
    "from urllib import request\n",
    "import pandas \n",
    "#pour les graphiques\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lecture de la page \n",
    "\n",
    "#On récupère le texte et l'id de la page suivante, qui apparait comme standardisé\n",
    "lien='http://www.sports.fr/football/ligue-1/2018/classements/4e-journee.html'\n",
    "request_text = request.urlopen(lien).read()\n",
    "page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyon\n"
     ]
    }
   ],
   "source": [
    "# page.find('div', {'class':'events'}).findAll('a')\n",
    "\n",
    "#s1=page.findAll('div',{\"class\": 'panel-body'})\n",
    "#s2=s1.findAll('table',{'class':'table table striped table-hover'})\n",
    "# s=page.findAll('td',{'class':'panel-body table table-striped table-hover lm1'})\n",
    "PageTime=page.find_all('tbody',class_='tc')\n",
    "PageTime2=PageTime[0].find_all('tr')\n",
    "#print(PageTime.text[4])\n",
    "#matrice=PageTime[0].find_all('td')\n",
    "#matrice[3].text\n",
    "#print(PageTime2[0].find_all('td')[3].text)\n",
    "L=len(PageTime2)\n",
    "l=len(PageTime2[0].find_all('td'))\n",
    "\n",
    "\n",
    "#création dico (je le fais sur la premiere journée)\n",
    "dicoéquipe = []\n",
    "liendereference='http://www.sports.fr/football/ligue-1/2018/classements/1e-journee.html'\n",
    "request_text2 = request.urlopen(liendereference).read()\n",
    "pagedereference = bs4.BeautifulSoup(request_text2, \"lxml\")\n",
    "\n",
    "for i in range(L):\n",
    "    page2=pagedereference.find_all('td',class_='tl nowrap')[i].text\n",
    "    page3=page2.replace('\\n', '').replace(' ','')\n",
    "    dicoéquipe.append(page3)\n",
    "#Fin création dico\n",
    "\n",
    "print(dicoéquipe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   4.  12.   4.   4.   0.   0.  14.   2.  12.   9.   3.   3.   0.\n",
      "    0.  11.   2.   3.   1.   1.   0.   0.   3.   0.]\n",
      " [  1.   5.  12.   4.   4.   0.   0.  14.   4.  10.   6.   2.   2.   0.\n",
      "    0.   9.   3.   6.   2.   2.   0.   0.   5.   1.]\n",
      " [  2.   7.   9.   4.   3.   0.   1.   5.   3.   2.   6.   2.   2.   0.\n",
      "    0.   4.   0.   3.   2.   1.   0.   1.   1.   3.]\n",
      " [  3.   0.   8.   4.   2.   2.   0.   9.   4.   5.   4.   2.   1.   1.\n",
      "    0.   7.   3.   4.   2.   1.   1.   0.   2.   1.]\n",
      " [  4.   8.   8.   4.   2.   2.   0.   9.   6.   3.   6.   2.   2.   0.\n",
      "    0.   4.   1.   2.   2.   0.   2.   0.   5.   5.]\n",
      " [  5.   1.   7.   4.   2.   1.   1.   6.   7.  -1.   4.   2.   1.   1.\n",
      "    0.   4.   1.   3.   2.   1.   0.   1.   2.   6.]\n",
      " [  6.   9.   6.   4.   1.   3.   0.   6.   4.   2.   2.   2.   0.   2.\n",
      "    0.   3.   3.   4.   2.   1.   1.   0.   3.   1.]\n",
      " [  7.  14.   6.   4.   2.   0.   2.   3.   2.   1.   3.   2.   1.   0.\n",
      "    1.   1.   1.   3.   2.   1.   0.   1.   2.   1.]\n",
      " [  8.   3.   6.   4.   2.   0.   2.   5.   6.  -1.   3.   2.   1.   0.\n",
      "    1.   2.   3.   3.   2.   1.   0.   1.   3.   3.]\n",
      " [  9.  12.   6.   4.   2.   0.   2.   8.  11.  -3.   6.   2.   2.   0.\n",
      "    0.   4.   2.   0.   2.   0.   0.   2.   4.   9.]\n",
      " [ 10.  11.   4.   4.   1.   1.   2.   4.   5.  -1.   1.   2.   0.   1.\n",
      "    1.   1.   2.   3.   2.   1.   0.   1.   3.   3.]\n",
      " [ 11.   6.   4.   4.   1.   1.   2.   3.   4.  -1.   4.   2.   1.   1.\n",
      "    0.   2.   1.   0.   2.   0.   0.   2.   1.   3.]\n",
      " [ 12.   2.   4.   4.   1.   1.   2.   4.   6.  -2.   3.   2.   1.   0.\n",
      "    1.   3.   2.   1.   2.   0.   1.   1.   1.   4.]\n",
      " [ 13.  19.   4.   4.   1.   1.   2.   4.   7.  -3.   3.   1.   1.   0.\n",
      "    0.   3.   0.   1.   3.   0.   1.   2.   1.   7.]\n",
      " [ 14.  17.   4.   4.   1.   1.   2.   1.   4.  -3.   1.   2.   0.   1.\n",
      "    1.   0.   1.   3.   2.   1.   0.   1.   1.   3.]\n",
      " [ 15.  18.   4.   4.   1.   1.   2.   5.  10.  -5.   3.   2.   1.   0.\n",
      "    1.   3.   5.   1.   2.   0.   1.   1.   2.   5.]\n",
      " [ 16.  13.   3.   4.   1.   0.   3.   3.   6.  -3.   3.   2.   1.   0.\n",
      "    1.   3.   2.   0.   2.   0.   0.   2.   0.   4.]\n",
      " [ 17.  16.   3.   4.   1.   0.   3.   3.   7.  -4.   3.   2.   1.   0.\n",
      "    1.   3.   2.   0.   2.   0.   0.   2.   0.   5.]\n",
      " [ 18.  10.   2.   4.   0.   2.   2.   6.   8.  -2.   1.   2.   0.   1.\n",
      "    1.   3.   4.   1.   2.   0.   1.   1.   3.   4.]\n",
      " [ 19.  15.   0.   4.   0.   0.   4.   1.   7.  -6.   0.   2.   0.   0.\n",
      "    2.   1.   4.   0.   2.   0.   0.   2.   0.   3.]]\n",
      "PSG\n",
      "Monaco\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#création du classement\n",
    "classement=np.zeros((L, l))\n",
    "\n",
    "#Je remplis les noms des équipes codés (ligne du dico)\n",
    "for i in range(L):\n",
    "    page2=page.find_all('td',class_='tl nowropen sqlite database\n",
    "                        ap')[i].text\n",
    "    page3=page2.replace('\\n', '').replace(' ','')\n",
    "    for j in range(L):\n",
    "        if dicoéquipe[j]==page3:\n",
    "            classement[i,1]=j\n",
    "            \n",
    "\n",
    "#Je remplis le reste\n",
    "for i in range(L):\n",
    "    Page1=page.find_all('td',class_='alt')#balise des points\n",
    "    classement[i,2]=Page1[i].text#je remplis les points\n",
    "    classement[i,0]=i\n",
    "    for j in range(3,l):\n",
    "        PageTime2=PageTime[0].find_all('tr')#balise du reste du tableau\n",
    "        classement[i,j]=PageTime2[i].find_all('td')[j].text\n",
    "print(classement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Comment itérer sur les variables pour crawler le site\n",
    "#Exemple\n",
    "matrice2 = []\n",
    "for i in range(10):\n",
    "    matrice2.append('abc'+str(i)+'bonjour')\n",
    "#Fin exemple\n",
    "\n",
    "#J'itère sur les liens (trois seulement)\n",
    "#On peut vérifier que le nombre de match joué correspond, ou recréer le nombre\n",
    "#de points de l'équipe seon ses résultats pour vérifier\n",
    "nombredeladernierejournéeàscraper=2\n",
    "for journée in range(1,nombredeladernierejournéeàscraper):\n",
    "    \n",
    "    lien='http://www.sports.fr/football/ligue-1/2018/classements/'+str(journée)+'e-journee.html'\n",
    "    request_text = request.urlopen(lien).read()\n",
    "    page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
    "\n",
    "    PageTime=page.find_all('tbody',class_='tc')\n",
    "    PageTime2=PageTime[0].find_all('tr')\n",
    "\n",
    "    L=len(PageTime2)\n",
    "    l=len(PageTime2[0].find_all('td'))\n",
    "#création du classement\n",
    "    classement=np.zeros((L, l))\n",
    "\n",
    "#Je remplis les noms des équipes codés (ligne du dico)\n",
    "    for i in range(L):\n",
    "        page2=page.find_all('td',class_='tl nowrap')[i].text\n",
    "        page3=page2.replace('\\n', '').replace(' ','')\n",
    "        for j in range(L):\n",
    "            if dicoéquipe[j]==page3:\n",
    "                classement[i,1]=j\n",
    "            \n",
    "\n",
    "#Je remplis le reste\n",
    "    for i in range(L):\n",
    "        Page1=page.find_all('td',class_='alt')#balise des points\n",
    "        classement[i,2]=Page1[i].text#je remplis les points\n",
    "        classement[i,0]=i\n",
    "        for j in range(3,l):\n",
    "            PageTime2=PageTime[0].find_all('tr')#balise du reste du tableau\n",
    "            classement[i,j]=PageTime2[i].find_all('td')[j].text\n",
    "\n",
    "np.savetxt(\"jaibienscrapé.csv\", classement, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\nPSG ', '\\n\\nMonaco ', '\\n\\nSaint-Etienne ', '\\n\\nLyon ', '\\n\\nBordeaux ', '\\n\\nMarseille ', '\\n\\nAngers ', '\\n\\nCaen ']\n"
     ]
    }
   ],
   "source": [
    "classement=[]\n",
    "equipe=[]\n",
    "pts = []\n",
    "j = []\n",
    "g = []\n",
    "n = []\n",
    "p = []\n",
    "bp = []\n",
    "\n",
    "page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
    "PageTest=page.find_all('tr')\n",
    "#type(PageTest[0].find_all('td',class_='lm1')[0].text) # ça marche\n",
    "#heure.append(PageTest[0].find_all('td',class_='lm1')[0].text)\n",
    "n=len(PageTest)\n",
    "L=len(PageTime2)\n",
    "for i in range(L):\n",
    "    classement.append(PageTest[i].find_all('td',class_='tr')[0].text)\n",
    "    equipe.append(PageTest[i].find_all('td',class_='tl nowrap')[0].text)\n",
    "    pts.append(PageTest[i].find_all('td',class_='alt')[0].text)\n",
    "    j.append(PageTest[i].find_all('td',class_='tl nowrap')[0].text)\n",
    "\n",
    "\n",
    "print(heure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   4.  12.   4.   4.   0.   0.  14.   2.  12.   9.   3.   3.   0.\n",
      "    0.  11.   2.   3.   1.   1.   0.   0.   3.   0.]\n",
      " [  2.   5.  12.   4.   4.   0.   0.  14.   4.  10.   6.   2.   2.   0.\n",
      "    0.   9.   3.   6.   2.   2.   0.   0.   5.   1.]\n",
      " [  3.   7.   9.   4.   3.   0.   1.   5.   3.   2.   6.   2.   2.   0.\n",
      "    0.   4.   0.   3.   2.   1.   0.   1.   1.   3.]\n",
      " [  4.   0.   8.   4.   2.   2.   0.   9.   4.   5.   4.   2.   1.   1.\n",
      "    0.   7.   3.   4.   2.   1.   1.   0.   2.   1.]\n",
      " [  5.   8.   8.   4.   2.   2.   0.   9.   6.   3.   6.   2.   2.   0.\n",
      "    0.   4.   1.   2.   2.   0.   2.   0.   5.   5.]\n",
      " [  6.   1.   7.   4.   2.   1.   1.   6.   7.  -1.   4.   2.   1.   1.\n",
      "    0.   4.   1.   3.   2.   1.   0.   1.   2.   6.]\n",
      " [  7.   9.   6.   4.   1.   3.   0.   6.   4.   2.   2.   2.   0.   2.\n",
      "    0.   3.   3.   4.   2.   1.   1.   0.   3.   1.]\n",
      " [  8.  14.   6.   4.   2.   0.   2.   3.   2.   1.   3.   2.   1.   0.\n",
      "    1.   1.   1.   3.   2.   1.   0.   1.   2.   1.]\n",
      " [  9.   3.   6.   4.   2.   0.   2.   5.   6.  -1.   3.   2.   1.   0.\n",
      "    1.   2.   3.   3.   2.   1.   0.   1.   3.   3.]\n",
      " [ 10.  12.   6.   4.   2.   0.   2.   8.  11.  -3.   6.   2.   2.   0.\n",
      "    0.   4.   2.   0.   2.   0.   0.   2.   4.   9.]\n",
      " [ 11.  11.   4.   4.   1.   1.   2.   4.   5.  -1.   1.   2.   0.   1.\n",
      "    1.   1.   2.   3.   2.   1.   0.   1.   3.   3.]\n",
      " [ 12.   6.   4.   4.   1.   1.   2.   3.   4.  -1.   4.   2.   1.   1.\n",
      "    0.   2.   1.   0.   2.   0.   0.   2.   1.   3.]\n",
      " [ 13.   2.   4.   4.   1.   1.   2.   4.   6.  -2.   3.   2.   1.   0.\n",
      "    1.   3.   2.   1.   2.   0.   1.   1.   1.   4.]\n",
      " [ 14.  19.   4.   4.   1.   1.   2.   4.   7.  -3.   3.   1.   1.   0.\n",
      "    0.   3.   0.   1.   3.   0.   1.   2.   1.   7.]\n",
      " [ 15.  17.   4.   4.   1.   1.   2.   1.   4.  -3.   1.   2.   0.   1.\n",
      "    1.   0.   1.   3.   2.   1.   0.   1.   1.   3.]\n",
      " [ 16.  18.   4.   4.   1.   1.   2.   5.  10.  -5.   3.   2.   1.   0.\n",
      "    1.   3.   5.   1.   2.   0.   1.   1.   2.   5.]\n",
      " [ 17.  13.   3.   4.   1.   0.   3.   3.   6.  -3.   3.   2.   1.   0.\n",
      "    1.   3.   2.   0.   2.   0.   0.   2.   0.   4.]\n",
      " [ 18.  16.   3.   4.   1.   0.   3.   3.   7.  -4.   3.   2.   1.   0.\n",
      "    1.   3.   2.   0.   2.   0.   0.   2.   0.   5.]\n",
      " [ 19.  10.   2.   4.   0.   2.   2.   6.   8.  -2.   1.   2.   0.   1.\n",
      "    1.   3.   4.   1.   2.   0.   1.   1.   3.   4.]\n",
      " [ 20.  15.   0.   4.   0.   0.   4.   1.   7.  -6.   0.   2.   0.   0.\n",
      "    2.   1.   4.   0.   2.   0.   0.   2.   0.   3.]]\n",
      "PSG\n",
      "Monaco\n"
     ]
    }
   ],
   "source": [
    "# page.find('div', {'class':'events'}).findAll('a')\n",
    "\n",
    "#s1=page.findAll('div',{\"class\": 'panel-body'})\n",
    "#s2=s1.findAll('table',{'class':'table table striped table-hover'})\n",
    "# s=page.findAll('td',{'class':'panel-body table table-striped table-hover lm1'})\n",
    "PageTime=page.find_all('tbody',class_='tc')\n",
    "PageTime2=PageTime[0].find_all('tr')\n",
    "#print(PageTime.text[4])\n",
    "#matrice=PageTime[0].find_all('td')\n",
    "#matrice[3].text\n",
    "#print(PageTime2[0].find_all('td')[3].text)\n",
    "L=len(PageTime2)\n",
    "l=len(PageTime2[0].find_all('td'))\n",
    "\n",
    "\n",
    "#création dico (je le fais sur la premiere journée)\n",
    "dicoéquipe = []\n",
    "liendereference='http://www.sports.fr/football/ligue-1/2018/classements/1e-journee.html'\n",
    "request_text2 = request.urlopen(liendereference).read()\n",
    "pagedereference = bs4.BeautifulSoup(request_text2, \"lxml\")\n",
    "\n",
    "for i in range(L):\n",
    "    page2=pagedereference.find_all('td',class_='tl nowrap')[i].text\n",
    "    page3=page2.replace('\\n', '').replace(' ','')\n",
    "    dicoéquipe.append(page3)\n",
    "#Fin création dico\n",
    "\n",
    "#création du classement\n",
    "classement=np.zeros((L, l))\n",
    "\n",
    "#Je remplis les noms des équipes codés (ligne du dico)\n",
    "for i in range(L):\n",
    "    page2=page.find_all('td',class_='tl nowrap')[i].text\n",
    "    page3=page2.replace('\\n', '').replace(' ','')\n",
    "    for j in range(L):\n",
    "        if dicoéquipe[j]==page3:\n",
    "            classement[i,1]=j\n",
    "            \n",
    "\n",
    "#Je remplis le reste\n",
    "for i in range(L):\n",
    "    Page1=page.find_all('td',class_='alt')#balise des points\n",
    "    classement[i,2]=Page1[i].text#je remplis les points\n",
    "    classement[i,0]=i+1\n",
    "    for j in range(3,l):\n",
    "        PageTime2=PageTime[0].find_all('tr')#balise du reste du tableau\n",
    "        classement[i,j]=PageTime2[i].find_all('td')[j].text\n",
    "print(classement)\n",
    "print(dicoéquipe[4])\n",
    "print(dicoéquipe[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMonaco '"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find_all('td',class_='tl nowrap')[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
